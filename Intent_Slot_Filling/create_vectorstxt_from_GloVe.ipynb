{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eab91318-cbb8-437e-bea0-38d38f9ed993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\arthu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "wn_lemmatiser = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7864055-b34c-4f16-95fc-a8a066130c17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3384c8b-688d-4d2a-8310-68f01bdfee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(sentence):\n",
    "  tokens = ''.join([char if ord('a') <= ord(char.lower()) <= ord('z') or char.isdigit() else ' ' for char in f'{sentence} '.replace(':','').replace(\"`\",\"'\").replace('pm ',' pm ')])\n",
    "  ts = []\n",
    "  for token in tokens.split():\n",
    "    if \"am \" in f'{token} ' and len(token) > 2 and token[-3].isdigit(): #avoid splitting words like ham, spam, sam, etc\n",
    "      ts.extend([token[:-2],\"am\"])\n",
    "    else:\n",
    "      ts.append(token)\n",
    "  return ts\n",
    "\n",
    "def normalise(sentence): \n",
    "  return [\"*\" * len(token) if token.isdigit() else wn_lemmatiser.lemmatize(token.lower(),'v') for token in tokenise(sentence)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f68dd6-d369-42a7-ae7a-fe4d6de11a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./data_dir/train.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5688593-5da6-46a9-9dac-a56397516ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['normalised'] = (df_train['sentence'].apply(lambda x : normalise(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e664961-feda-4ad5-903d-71f7eb718a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('my_corpus','a') as f:\n",
    "  for elm in df_train['normalised'] :\n",
    "    f.write(' '.join(elm)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca64c6a5-8171-4d3b-97b9-1a45e2e855dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GloVe'...\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/stanfordnlp/GloVe.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e39dd1-178a-4289-b0b8-fcd20cdc0f25",
   "metadata": {},
   "source": [
    "# Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99024e34-36f0-444e-bddd-e41a06428f33",
   "metadata": {},
   "source": [
    "One NEEDS to go into the GloVe directory, modify demo.sh by setting : \\\n",
    "CORPUS:{path of 'my_corpus'} \\\n",
    "VOCAB_MIN_COUNT=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87f782e-71a1-4210-9d65-868e821c2e69",
   "metadata": {},
   "source": [
    "After it is easier on a Unix system, that is why I used Google Colab for this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1fa24-e147-4afb-aa33-7dea31e2cb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "! cd GloVe && make\n",
    "! cd GloVe && ./demo.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
